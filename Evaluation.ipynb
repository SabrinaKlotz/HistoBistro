{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "import pytorch_lightning as pl\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from classifier import ClassifierLightning\n",
    "from options import Options"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = Path('18-LSS0736')\n",
    "feature_path = Path('/Users/sophia.wagner/Downloads/439097.h5')\n",
    "model_path = Path('/Users/sophia.wagner/Documents/PhD/projects/2022_MSI_transformer/attention-user-study/multi-all-same_transformer_DACHS-QUASAR-RAINBOW-TCGA_histaugan_isMSIH/models/best_model_multi-all-same_transformer_DACHS-QUASAR-RAINBOW-TCGA_histaugan_isMSIH_fold3.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_file = h5py.File(feature_path)\n",
    "features = torch.Tensor(np.array(h5_file['feats'])).unsqueeze(0)\n",
    "coords = torch.Tensor(np.array(h5_file['coords']))\n",
    "coords = [(coords[i, 0].int().item(), coords[i, 1].int().item()) for i in range(coords.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- load options ---\n",
      "bs: 1\n",
      "clini_info: {}\n",
      "cohorts: ['TCGA']\n",
      "criterion: BCEWithLogitsLoss\n",
      "data_config: /home/ubuntu/projects/idkidc/data_config.yaml\n",
      "debug: False\n",
      "ext_cohorts: ['CPTAC']\n",
      "feats: ctranspath\n",
      "folds: 5\n",
      "input_dim: 768\n",
      "lr: 2e-05\n",
      "model: transformer\n",
      "name: test\n",
      "norm: macenko\n",
      "num_classes: 1\n",
      "num_epochs: 1\n",
      "num_tiles: -1\n",
      "optimizer: AdamW\n",
      "pad_tiles: False\n",
      "project: hackathon\n",
      "resume: None\n",
      "save_dir: /home/ubuntu/logs\n",
      "scheduler: None\n",
      "seed: None\n",
      "stop_criterion: loss\n",
      "target: isMSIH\n",
      "task: binary\n",
      "wd: 2e-05\n"
     ]
    }
   ],
   "source": [
    "parser = Options()\n",
    "args = parser.parser.parse_args('')  \n",
    "\n",
    "# Load the configuration from the YAML file\n",
    "with open(args.config_file, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Update the configuration with the values from the argument parser\n",
    "for arg_name, arg_value in vars(args).items():\n",
    "    if arg_value is not None and arg_name != 'config_file':\n",
    "        config[arg_name]['value'] = getattr(args, arg_name)\n",
    "\n",
    "# Create a flat config file without descriptions\n",
    "config = {k: v['value'] for k, v in config.items()}\n",
    "\n",
    "print('\\n--- load options ---')\n",
    "for name, value in sorted(config.items()):\n",
    "    print(f'{name}: {str(value)}')\n",
    "\n",
    "cfg = argparse.Namespace(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.pos_weight = torch.tensor([1.0])\n",
    "classifier = ClassifierLightning(cfg)\n",
    "checkpoint = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
    "checkpoint['state_dict'].keys()\n",
    "classifier.load_state_dict(checkpoint['state_dict'])\n",
    "classifier.eval();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load plotting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that plots scores nicely. scores should have the same length as the number of tiles.\n",
    "\n",
    "def plot_scores(coords, scores, image, overlay=True, clamp=0.05, norm=True, colormap='RdBu', crop=False):\n",
    "    if clamp:\n",
    "        q05, q95 = torch.quantile(scores, clamp), torch.quantile(scores, 1-clamp)\n",
    "        scores.clamp_(q05,q95)\n",
    "    \n",
    "    if norm:\n",
    "        scores = NormalizeData(scores)\n",
    "        \n",
    "    if crop:\n",
    "        coords_min, coords_max = np.array(coords).min(axis=0), np.array(coords).max(axis=0)\n",
    "        y_min, y_max, x_min, x_max = round(coords_min[1]/d), round(coords_max[1]/d), round(coords_min[0]/d), round(coords_max[0]/d)\n",
    "        if pat_name == '439042':\n",
    "            x_max = round((69 * 1013)/d)\n",
    "        print(y_min, y_max, x_min, x_max)\n",
    "    else:\n",
    "        y_min, y_max, x_min, x_max = 0, image.shape[0], 0, image.shape[1]\n",
    "\n",
    "        \n",
    "    attention_map = np.zeros((image.shape[0], image.shape[1]), dtype=np.float32)    \n",
    "    tissue_map = -np.ones((image.shape[0], image.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    offset = 1013\n",
    "    for (x,y), s in zip(coords, scores):\n",
    "        \n",
    "        if colormap == 'RdBu': \n",
    "            attention_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = 1 - s.item()\n",
    "        else: \n",
    "            attention_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = s.item()\n",
    "        tissue_map[round(y/d):round((y+offset)/d), round(x/d):round((x+offset)/d)] = s.item()\n",
    "            \n",
    "    attention_map = np.array(attention_map * 255., dtype=np.uint8)\n",
    "    tissue_map[tissue_map>=0] = 1\n",
    "    tissue_map[tissue_map<0] = 0\n",
    "    \n",
    "#     plt.figure(figsize=(30, 30))\n",
    "    a = 1.\n",
    "    if overlay:\n",
    "        plt.imshow(image[y_min:y_max, x_min:x_max])\n",
    "        a = 0.5\n",
    "\n",
    "    if crop:\n",
    "        plt.imshow(attention_map[y_min:y_max, x_min:x_max], alpha=a*(tissue_map[y_min:y_max, x_min:x_max]), cmap=colormap, interpolation='nearest')\n",
    "#         plt.imshow(attention_map[round(coords_min[1]/d):, round(coords_min[0]/d):], alpha=a*(tissue_map[round(coords_min[1]/d):, round(coords_min[0]/d):]), cmap=colormap, interpolation='nearest')\n",
    "    else:\n",
    "        plt.imshow(attention_map, alpha=a*(tissue_map), cmap=colormap, interpolation='nearest')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load attention utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rollout_attention(all_layer_matrices, start_layer=0):\n",
    "    # adding residual consideration- code adapted from https://github.com/samiraabnar/attention_flow\n",
    "    num_tokens = all_layer_matrices[0].shape[1]\n",
    "    batch_size = all_layer_matrices[0].shape[0]\n",
    "    eye = torch.eye(num_tokens).expand(batch_size, num_tokens, num_tokens).to(all_layer_matrices[0].device)\n",
    "    all_layer_matrices = [all_layer_matrices[i] + eye for i in range(len(all_layer_matrices))]\n",
    "    matrices_aug = [all_layer_matrices[i] / all_layer_matrices[i].sum(dim=-1, keepdim=True)\n",
    "                          for i in range(len(all_layer_matrices))]\n",
    "    joint_attention = matrices_aug[start_layer]\n",
    "    for i in range(start_layer+1, len(matrices_aug)):\n",
    "        joint_attention = matrices_aug[i].bmm(joint_attention)\n",
    "    return joint_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout(model, input, start_layer=0):\n",
    "    model(input)\n",
    "    blocks = model.transformer.layers\n",
    "    all_layer_attentions = []\n",
    "    for blk in blocks:\n",
    "        attn_heads = blk[0].fn.get_attention_map()\n",
    "        avg_heads = (attn_heads.sum(dim=1) / attn_heads.shape[1]).detach()\n",
    "        all_layer_attentions.append(avg_heads)\n",
    "    rollout = compute_rollout_attention(all_layer_attentions, start_layer=start_layer)\n",
    "    return rollout[:,0, 1:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollout = generate_rollout(classifier.model, features, start_layer=0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_scores(coords, rollout, clamp=0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = rollout.topk(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17221, 62806)\n",
      "(10130, 50650)\n",
      "(18234, 61793)\n",
      "(18234, 35455)\n",
      "(16208, 61793)\n"
     ]
    }
   ],
   "source": [
    "for i in indices:\n",
    "    print(coords[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.0769e-04, 3.8338e-04, 9.3652e-05,  ..., 9.9520e-05, 9.9313e-05,\n",
       "        1.0548e-04])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
